\documentclass[12pt]{article}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{setspace}
\usepackage{indentfirst}
\usepackage{listings}
\usepackage{mathpazo}
\usepackage[T1]{fontenc}
\linespread{1.6}
\begin{document}

%Title page

\begin{titlepage}
    \begin{center}
        
        \begin{doublespace}        
        \textbf{\Large Navigating Articles of News Websites With a Web Metrics Visualization}
        \vfill

        \vspace{2in}
        \textbf{\Large Alain Ibrahim} \\
		\vspace{2in}
        A Thesis in the Field of Information Technology \\
        for the Degree of Master of Liberal Arts in Extension Studies \\

        \vspace{.5in} 

        
        Harvard University \\
        \vspace {.4in}
        March 2015        

		\end{doublespace}        
        
        \vfill
         
    \end{center}
\end{titlepage}

%Abstract

\section{Abstract}

Current prominent web traffic analytics systems such as Google Analytics report traffic per website, given certain filtering criteria. Such systems are typically geared to system administrators who use the data to identify patterns and possibly assist management in strategy making. For news sites, this may mean analyzing the traffic on a news article and where most of the readers came from. The end users, however, will at best see metrics such as social likes and shares within the visited article. But, what if we could provide the end users a near real-time metrics map of the entire news site they are visiting? What if they could navigate based on traffic patterns, as opposed to having to follow the common front page layout specified by the news editors? This is what I built - a system that hooks into a Wordpress-based news website, captures the visitors' web metrics, and conveys the near real-time traffic through an interactive web-based visualization.
\newpage

\tableofcontents

\vfill

\section{Thesis Project Description}
In 2015, the web of online media can be divided into 2 categories: News websites that contain facts about world events and social media sites that cover the ongoing conversations around world events. In the pre social media days, the news information flow was one-directional - with hardly any feedback from the site visitors. However, in the social media age, visitors now share articles in their online social properties such as Facebook and Twitter. Yet, what appears on the front page of one's favorite news site is not necessarily reflective of what he/she wants to see; it is set by the web news editorial team and can be influenced by the station's political or business agenda. My goal in this thesis was to create a different way of seeing the news, using an interactive web visualization. The product is geared to visitors that are curious in knowing how long an article was popular for and how interested the readers were when they read that article. At the same time, these measures could be visually compared with those of other articles for a relative analysis of popularity and level of interest. 

Different news stations use different web content management systems (WCMS). The system that I built hooks into a ubiquitous and open source WCMS - Wordpress. It then captures web metrics, and visualizes these metrics inside an interactive web-based visualization. The visualization is targeted towards site visitors but can also be used by web admins. There are 3 main pieces of data I was interested in: The number of hits to a an article, the amount of time spent reading an article, and the geographic location of the visitor. 

In the first stage, I created a database to house the web metrics that I wanted to capture. I then created a server-side library that can be included into any website or web application. This library hooks into my database and efficiently captures the hits' metadata against the given news site. Such metadata includes time of article visit, region of visitor, and timezone of visitor.  
I then built the "business logic" of the data and information that I wanted to capture. This includes algorithms that produce the refined data and metrics to be ultimately communicated to the end user.
This amounted to the final step of creating an interactive visualization that shows near real-time user web traffic in each of the registered articles of the connected news site.

\newpage

The domain that I targeted is \textbf{online news media}. I captured the below data, as far back as 31 days:
\begin{itemize}
\item The number of visitor hits to a given article. Each hit is defined by a unique timestamp and IP address
\item The visitor's geographic location
\item The amount of time a visitor spent reading the article for a given hit
\end{itemize}


\noindent The interactive visualization enabled 4 tasks:
\begin{enumerate}
\item \textbf{To compare articles' hits and reader attention spans for any period within the last month, given a select category}
\item \textbf{To preview each article's thumbnail, title, and excerpt}
\item \textbf{To contrast the geographic distribution of hits and attention of up to 2 articles}
\item \textbf{To navigate to the full article counterpart on the subject news website}
\end{enumerate}

\noindent{As for interactivity, the visualization enabled the following:}
\begin{itemize}
\item Hovering over article elements to preview the thumbnail, title, and excerpt of the body content
\item Clicking on an article element to display a table that shows the geographic distribution of the number of hits and attention span for that article
\item Filtering the data via a controls box. The controls box allows the user to select a desired news category, to filter by date range, and to toggle context-related settings 
\end{itemize}

The final product is geared towards the conventional news site structure, where categories are enumerated (e.g. sports, health, science, politics, etc.), and where articles are listed under their pertinent categories. \\

\noindent\includegraphics[scale=0.4]{img/in_action} \\

\noindent\textbf{Figure 1 -- The user previews a music article in the interactive visualization while having selected 2 articles for hits and attention span comparison.}


\newpage

\noindent{Here are the technologies that I used in building my system:}
\begin{itemize}
\item MySQL for the relational database that houses the web metrics
\item PHP for the server-side script that stores and retrieves the data from the database
\item HTML 5 for laying out the basic components of the visualization - for example, the controls box
\item SVG to markup the visualization elements
\item CSS3 for styling the HTML and SVG elements
\item JavaScript to manipulate the DOM (Document Object Model) and to load data from the server side
\item D3.js (Data-Driven Documents) to facilitate the manipulation of the data in the visualization
\item jQuery to shorthand some lengthy JavaScript functions
\end{itemize}


%I built the system in the following chronological order:
%\begin{enumerate}
%\item Abstracted the web traffic data that needs to be captured and develop the business model. Specifically, I first defined what data was useful and how to develop the algorithms to capture it
%\item Built a relational database in MySQL around the results from 1)
%\item Developed server-side classes with PHP that captured and retrieved the collected data
%\item Researched and experimented with different visualizations that could best fit 2)
%\item Developed and implemented the client-side visualization using D3
%\end{enumerate}

\vfill

\section{Prior Work}
\subsection{Visualization}
Prominent researchers in the field of visualization have concocted logical constructs and approaches to designing and implementing visualizations. From a presentation standpoint, there is much emphasis on presenting the end user just enough visual data for the user to achieve the intended task of the visualization - this is known as \textbf{expressiveness} \cite{schumann} - where the visualization shows information only relevant to the data set. Another popular concept on visual economics is the \textbf{data-to-ink ratio} \cite{tufte}. The goal is to have just enough ink presented to convey the data; additional ink that is not matched by data is a distraction. 

Having an efficient visualization is not enough. It needs to be effective. This brings us to two concepts covered in depth by Tamara Munzner: \textbf{domain} and \textbf{task} (Munzner \cite{munzner}). She advocates that in the design phase, the builder needs to identify the domain that he/she is working with. A domain is a world of ideas and concepts that are abstracted under one umbrella. For example, human anatomy, software engineering, economics, and biology are all domains. A domain gives rise to concepts, definitions, and a pre-built library of shapes and colors by the convention of collective knowledge. Red in biology signifies blood whereas in the world of novels it may allude to romance. Thus the target domain largely determines what colors and shapes the visualizer will have on their palette. \\
On the other hand, the task refers to the "what" of the visualization. What is/are the goal(s) of the visualization? Is it to explore, analyze, extrapolate, etc.? Clearly identifying the task(s) of the visualization at hand solidifies the direction in which the designer \textbf{encodes} the visual variables (Bertin). Encoding refers to the idea of representing a quantitative or qualitative concept visually. The visual variables include position, size, shape, value, color, orientation, grain, and texture. \\
Thus for an effective visualization, the choice of encodes is dependent on the domain of the visualization, as as well as the tasks the visualization is trying to support. \\
Studies have shown that position and length are the easiest visual variables to decode (Cleveland and McGill). My project deals with two main measures: Number of hits and read time (in seconds). To incorporate position, I can plot these two measures on a Cartesian graph with time on the X axis and the selected measure on the Y axis. If I plot each datum as a dot or cirle, I end up with a scatterplot. Though, my goal is show trends of popularity; which is why a line chart and area graph are better candidates as they convey patterns of over time. Comparing the total number of hits across different geographic locations involves encoding using length. Here, a bar chart suits the task. Lastly, visualizing the number of seconds for each observation given a geographic location over a specified time period is best accomplished using position; for this, I can use a dot chart. \\
For distinguishing categories in a line or graph chart, color is an effective visual variable. For this project, news categories are not simultaneously contrasted - articles are. Encoding each article with a different color may work well for up to 5 colors; however, studies show that surpassing this limit makes it difficult for humans to differentiate encodes of more than 7-9 colors (Healey). Thus a different visual variable will need to be applied to effect in the visitor distinguishing the articles from one another. Here, I can resort to the visual variable of position. A stacked area graph is an area graph that employs position by stacking different parts of the whole on top of one another over another dimension - usually time (Byron \& Wattenberg).  

In the pre-Internet days, the implementation would have ended here. Though, with the advent of mass computerization and global connectivity, a new ability has been introduced - that of interactivity. Interactivity has been broken down into the following \cite{cockburn}: Overview+Detail, Zooming, Focus+Context, details-on-demand, and cue-based systems. The user is now able to manipulate the views of the data using input devices such as a mouse and keyboard. As with the other aspects of the visualization, efficiency and effectiveness need to be kept in mind when creating user interfaces.

\subsection{Measuring Time}
Time is an essential component to web analytics, as it gives context to what is being measured. Although graphing change over time was attempted in the early A.D.s, the first known contemporary time series graph was published by William Playfair \cite{playfair}. Playfair was the father of the line graph and bar graph. However, there is not a single model that accommodates all domains and tasks \cite{frank}. Thus, for the proposed visualization, time must be dealt with in the context of an online news domain.   

Here, it is important to note the difference between event data and interval data. Time events are occurrences that are represented as slices of time; for example, a plane landed at its destination is an event. Whereas, the process of a plane landing occurs over a time interval of say 20 minutes. In this project, an article hit is defined as an event, although an actual hit has a visit duration. That said, if I visualize time on an X axis it is inevitable that I will have units of times where no hits are available. The rendering of this time interval as a discrete event is adequate so long as the measured interval is minute compared to the total time covered by the data (Graham). That is, a visitor reading an article for 2 minutes is a small amount compared to say an article's lifetime of 3 weeks. 

In order to have a better understanding of the time intervals of news, I asked professionals around the news station that I work at for feedback. Based on this and my observations of our news website and other similar entities' websites, "news" refers to a recent occurrence, notably something that happened today. "Breaking News" typically refers to an event that happened within the current day and whose story under development. "Recent News" pertains to stories created within the last 1-3 days. As time approaches the coming year, decade, century, and millennium, news are then compiled and shown in representations such as "year/decade/century/millennium in review". My goal in this project is to represent the "recent news" category. Thus, the needed time granularities to be conveyed that support the task of the proposed visualization should be in days \cite{bettini}. At a minimum for drilling down to a day's worth of news, I need to use a discrete time scale \cite{goralwalla} and thus need to capture the day, hour, and minute of each visit. The output variables connected to time should be the number of hits and the average read time spent on an article.

\subsection{Existing web metrics systems}
What I built is novel in that there is no current navigable and exploratory tool that visualizes near real-time traffic on news sites. \\
Here are a few systems that I found which deal with visualizing web metrics: \\

\noindent\textbf{Flow (http://www.webresourcesdepot.com/beautiful-free-website-traffic-visualization-application-flow/)} \\ \\
\noindent\includegraphics[scale=0.45]{img/flow}
\noindent\textbf{Figure 6 -- Shows a line graph that measures the number of hits on the y axis, and date on the x axis for select categories on a given website.} \\

While this can be useful in providing some feedback to the news site's administrators, it does not provide detailed insight to the articles in a given news site. The below screenshot demonstrates another visualization part of Flow. \\

\vfill

\noindent\includegraphics[scale=0.7]{img/flow1} \\
\noindent\textbf{Figure 7} \\

\newpage


\noindent\textbf{Google Analytics} \\ \\
\noindent\includegraphics[scale=0.4]{img/google_analytics} \\
\noindent\textbf{Figure 8 -- shows the Real-Time metrics pane in Google Analytics.} \\

Google Analytics comes in handy in aggregating data and conveying statistical data to the end user. The end user here would navigate here with the intention of running an analysis, such as: From what parts of the world is most of this website's traffic coming from? What links are being most visited?
While this is a great tool for management, it is unlikely that the end users will wake up to their coffee looking at this, and then navigating to the actual news website. \\

\newpage

\noindent\textbf{Adobe Analytics} \\
\noindent\includegraphics[scale=0.6]{img/adobe_analytics}


Adobe Analytics covers a slew of reporting capabilities:

\begin{itemize}
\item \textbf{Marketing reports and analytics.} Includes dashboards and reports, conveying key performance indicators, showing marketing strategies' performance in real-time, and displaying the results of ad campaigns
\item \textbf{Ad hoc analysis.} Includes multidimensional site analysis and advanced reporting
\item \textbf{Data workbench.} Involves collecting, processing, analyzing, and visualizing Big Data from online and offline customer interactions
\item \textbf{Predictive analysis.} Pertains to discovering hidden customer behavior
\item \textbf{Real-time web analytics.} Shows what is happening on the digital properties in real-time so that managers can take action where needed
\end{itemize}

Once again, the intended audience is that of management and decision makers. What I plan to create is something that the news consumers could see and use to make decisions on what to navigate to.

\null
\vfill


\section{Implementation - data capture}
\subsection{Capturing Data}
\subsubsection{IP Addresses}
I captured the IP addresses of the visitors to the subject news site. In PHP, an associative array called \textbf{\$\_SERVER}  contains several variables that pertain to the HTTP connection between the client and the server. The variable that contains the IP address from which the user is viewing a select page is called \textbf{REMOTE\_ADDR}. In code, I registered the visiting IP address like so:
\begin{lstlisting}
$user_ip = $_SERVER["REMOTE_ADDR"];
\end{lstlisting}

Here, I would like to shed light on the difference between static and dynamic IP addresses. The former is set by the network administrator and is fixed unless it is changed manually. The latter is assigned by the network router, and is assigned for a temporary period called the DHCP lease time. In order to find out the average DHCP lease time set by Internet Service Providers (ISPs), I asked a few network engineers and contacted 3 prominent ISPs (Internet Service Providers). Here are the results:
\vspace{0.3in}

\begin{tabular}{| c | c |}
  \hline                       
  Network Engineer 1 & 7 days \\[1ex] \hline
  Network Engineer 2 & 3 days \\[1ex] \hline
  Network Engineer 3 & 3 days \\ [1ex] \hline
  Verizon (ISP) & 14 days \\[1ex] \hline
  Comcast (ISP) & 4-7 days \\[1ex]
  \hline  
\end{tabular}
\vspace{0.3in}

The average DHCP lease time is between 3 and 7 days. In capturing hits, the threshold of the time limit where a new visit would be counted as a new one practically ranges from seconds to hours. For this reason, I can hold the distinction between static and dynamic IP addresses as constant - as the time revisit threshold is less than the average renew period of at least 1 day. This is important when defining a hit is mainly based on a visit from a unique IP address. The IP address is the only means to track a user and that does not require user authentication. 

As for user integrity, one cannot fully ensure that a physical human is sitting behind an IP address when automated scripts may be hitting the IP address of a given news article. Though, my goal is to minimize the inaccuracy when a visitor reaches a news article. I countered this by registering the visiting IP address as soon as the user has moved his/her mouse, or, when he/she has scrolled at least once when viewing the article at hand. I first tested a stub for this successfully in \textbf{register\_ip.php}. Here, I have a page containing several blocks of repeated Lorem ipsum text. I used JavaScript to listen to the mousemove and scroll events. I used jQuery to shorthand the event bindings. Below is the code:
\begin{lstlisting}[basicstyle=\scriptsize]
mouse_moved = false;
page_scroll_counter = 0;

$("body").on("mousemove",function(){
	if(!mouse_moved) {
		\\ Here, a flag would be sent to the server via AJAX
		console.log("Mouse has moved");
		mouse_moved = true;
	} else {
		
	}
});

$(document).on("scroll",function(){
	if(page_scroll_counter == 1) {
		\\ Here a flag would be sent to the server via AJAX
		console.log("Page has been scrolled")
	} else {
		page_scroll_counter++
	}
});
\end{lstlisting}        

\newpage

\subsubsection{Geographic origins}
Another metrics that I captured was the user geographic distribution for a select news article. This required two steps:
\begin{enumerate}
\item Capturing the IP address of the user. This was demonstrated in the previous section
\item Obtaining the corresponding latitude and longitude coordinates associated with the user's IP address
\end{enumerate}

Here, it is important to note that many users may be sitting behind HTTP proxy servers. HTTP proxy servers are physical computers/servers that act as proxy points for the client computers. For example, if someone in Russia used a proxy server in the US to visit my news article, the requesting IP address that will be captured will be that of the US - making it appear that the user that visited the article actually came from the US. In PHP, in addition to \textbf{\$\_SERVER["REMOTE\_ADDR"]}, there is another variable called \textbf{\$\_SERVER["HTTP\_X\_FORWARDED\_FOR"]}. While the former gets the IP address of the direct requester, the latter gets the originating IP that is making the request. So for example, if I were to be sitting behind an IP proxy server which had an address of 111.111.111.111, then \$\_SERVER["REMOTE\_ADDR"] would carry that value. \\
\$\_SERVER["HTTP\_X\_FORWARDED\_FOR"] would then contain the value of my router's outside IP address - which is what I am looking to map geographically.   
\\ If on the other hand the user was browsing the Internet without the use of a proxy server, then \$\_SERVER["REMOTE\_ADDR"] would carry the value of the user's router's IP address - reflective of the user's geography. 
Here is the methodology that used to capture the needed IP address for the geographic lookup:
\begin{enumerate}
\item Get the value of \$\_SERVER["REMOTE\_ADDR"] and look up its value with a geolocation API. Call this \textbf{X}
\item Get the value of \$\_SERVER["HTTP\_X\_FORWARDED\_FOR"] and look up its value with a geolocation API. Call this value \textbf{Y}
\item If X and Y are both available, it means that the user is sitting behind a proxy and that the needed IP address is that of Y. If X is available but Y is not, it means that Y is a private IP address, that of the device behind its router - and thus not behind a proxy server. In this case, X would hold the IP address of the origin.
\end{enumerate} 

The next step here is to find a service that would map a given IP address to its corresponding geo coordinates, and ideally to its country name. A quick Google search revealed a convenient REST API service that would output the responses in JSON. \textbf{Telize (www.telize.com)} is a REST API that allows one to get the visitor IP address and to query the location information for that IP address. The upside of this service is that at the moment it contains to no rate limits; thus, one could make an indefinite amount of calls to the Telize API. I was able to test the above clause 3 in PHP code with success. Regardless, of whether I was sitting behind a proxy server or not, the output showed the United States as the country of origin, which means that my code worked.

\subsubsection{Average Read Time}

One metric that I was curious in attaining was how long the average user spends reading a certain news article. At the moment, a cousin measure called the bounce rate checks to see whether the user continues to browse other pages in the site, or whether the user 'bounces' off to other websites. I find this measure to not be indicative of the level of interest the reader has for a given news article. I plan to quantify the interest level by doing the following:
\begin{enumerate}
\item Research the average reading speed of adults that can read English
\item Count the number of words in a given news article
\item Based on the average read speed, calculate the time spent on the news article. If the page is kept open indefinitely by the reader, I will simply use the maximum expected read time for that article. While this not 100\% accurate, it will be highly reflective of the level of interest with the visited articles. 
\end{enumerate}     

\noindent Based on an ophthalmology study that consisted of 50 native English-speaking individuals (average age 30.38$\pm$9.44 years; 16 university students, 18 academics, and 16 non-academics) -- the average reading speed came out to be 201.53$\pm$35.88 words per minute(wpm) for short sentences, and 215.01$\pm$30.37 wpm for long paragraphs\cite{radner}. For the scope of this project, I will use the average of 200 wpm to calculate the maximum average time spent on a news article.  

\vfill

\subsection{Database Design}
\subsubsection{Data Storage}
Based on the data that needed to be captured, the below fields were created:
\begin{itemize}
\item \textbf{Visitor IP address}: Stores the IP address of the visitor reading the news article. In order to avoid capturing page refreshes as new sessions, I will set a threshold of 10 minutes - whereby the server checks if the visitor has visited the article in the past 10 minutes
\item \textbf{Time Visited}: Stores a timestamp (in GMT) of when the user visited 
\item \textbf{Timezone}: Helpful in determining time differences from GMT
\item \textbf{Country}: Stores the name of the country where the visitor is from
\item \textbf{Region}: Stores the region pertinent to the area within the country where the visitor is from
\item \textbf{Read Time}: Stores the time taken for the reader to have finished reading the article. This should be calculated in seconds
\item \textbf{Expected Read Time}: Stores the maximum time it should have taken the reader to finish the words in the body of the article
\item \textbf{Article ID}: Stores the unique identifier and is captured from the host's CMS. This way, future references could be made easily without having to go through mapping algorithms
\item \textbf{Article Publish Date}: Stores the timestamp of when the article was published
\item \textbf{Category ID}: Stores the unique identifier of the category, also as it appears in the corresponding CMS
\item \textbf{Article URL}: Stores the article URL 
\item \textbf{Article title}: Stores the title of the article as it appears in the CMS or on the news site
\item \textbf{Sample text}: Stores an excerpt of the text from the body of article
\item \textbf{Sample picture}: Stores a URL to the thumbnail picture of the article

\end{itemize}

\subsubsection{Database Tables}
I adhered to the principles of relational database normalization in order to maximize efficiency and eliminate redundancy. For the scope of this project, all the data pertinent to a web hit was stored in a single table: \\ 
{\large\textbf{hit (\underline{id},ip, time\_visited, \textit{article\_id}, timezone, country, region, read\_time)}} \\ \\
The underline signifies the primary key. The primary key is the field(s) that are enough to determine the rest of the information for any row instance. In this case, the IP address, article ID, and time of visit jointly determine the timezone, country, region, and read time. However, since I need a way to track the user's visits to capture read time, I designated an auto-increment unique identifier called \textbf{id}. \textit{article\_id} is italicized since it is a foreign key -- that is, it represents a primary key in another table. The time visited is rounded to hour:minute. Therefore, seconds are not be accounted for in the visit's time stamp. The read time defaults to the maximum read time as calculated by the article's length. This assumes that the user stayed on the article page indefinitely. In the event that the user does leave the page, this value would then register the actual read time.
\\ \\
The second table needed contained all the metadata pertinent to the visited article:\\
{\large\textbf{article (\underline{article\_id}, publish\_date, category\_name, article\_url, title, sample\_text, sample\_pic, expected\_read\_time)}} \\ \\

The third table needed contained metadata pertinent to the category of the visited article:\\
{\large\textbf{category (\underline{category\_id}, category\_name})}
\\ \\

It is important to note here that all of above data is communicated via the server side script which is included in the code of each loaded article. 

\subsubsection{MySQL Database}
For this project, I used MySQL since I have good experience with it and for its being open source. MySQL is a relational database management system. Relational databases are those that adhere to relational database design, which relies on the fundamentals of relational algebra and relational calculus. The main tenet of relational design is to maximize efficiency and eliminate any redundancy in the data being stored.

I created a database called \textbf{alaini\_news\_viz} on my web host. For this, I created two users:\\
\begin{itemize}
\item \textbf{alaini5\_newsread}: This account is solely used to read from the database. Ultimately, it will be used to invoke the visualization by the visitor/end user
\item \textbf{alaini5\_newsedit}: This account is mainly used to register the IP hits and read times communicated to the server
\end{itemize}

I created 3 tables in MySQL using phpMyAdmin, an open source PHP based GUI that facilitates interacting with the MySQL system:
\begin{enumerate}
\item{\textbf{article}}\\ \\
\includegraphics[scale=0.6]{img/article_table}
\item{\textbf{category}}\\ \\
\includegraphics[scale=0.6]{img/category_table}
\item{\textbf{hit}}\\ \\
\includegraphics[scale=0.6]{img/hit_table}
\end{enumerate} 

\subsection{PHP server-side code}
\subsubsection{Configuration and database connection}
Here, I created two files:
\begin{enumerate}
\item{\textbf{constants.php:}} Holds all the constants that are defined across the application, inclusive of those for the database connection
\item{\textbf{db\_connect.php:}} Performs the PDO database connection based on the given connection parameters
\end{enumerate}
Note that both of these files are stored one directory above \textbf{public\_html/} for security reasons. This folder is called \textbf{lib/}. Thus, the two files are not accessible directly to the public and accordingly need to be included in other PHP files.

\subsubsection{Server-side hit, article, and category capture}
Each table in my database is represented with a PHP class. Hence, the creation of the following 3 classes:
\begin{enumerate}
\item{\textbf{Hit.php:}} This class registers all of the metadata pertinent to a user visit. This data includes IP address, country, region, timezone, and time visited. 
\vfill
\item{\textbf{Article.php:}} This class captures all of the metadata pertinent to the article being visited. Such includes title, text excerpt, thumbnail URL, and article URL.
\vfill
\item{\textbf{Category.php:}} This class captures any metadata pertinent to the parent category of the visited article. At the moment, I am only capturing the name of the category.
\end{enumerate}
\vfill

In addition to the above, I have created a helper file that registers the read time of a given article. By default, the read time is registered as 1 second in the database. Once a hit is registered in the database, the id of the newly created hit is then passed to the DOM of the article page. There, the id is used via an AJAX call to store the calculated read time for the visited article in the database. This file is called \textbf{updateReadTime.php}.
\vfill

\subsection{Client-side read time capture with JavaScript}
As mentioned in the last section, the hit id is passed to the client page in order to keep a way to identify and update the read time for the visited article. Here, I calculate the number of words that are stored in the article. I do this by first getting all of the text found within common article tags - namely <span>, <div>, <article>, <p>, and <section>. I then sum the number of words found within all of the mentioned tags. I ensure that double counting is avoided by using the \textbf{innerText} property; this counts the elements inner text, but not any text that might be included in one of its children nodes. Next, I calculate the number of seconds the article is on average expected to take to read. Based on my earlier research, I will use the average figure of ~200 words per minute. This figure is used in the event that a user leaves the page open indefinitely. Thus, if an article should take about 5 minutes to read, and the user takes (for whatever reason) 30 minutes to navigate away from or to close the article, then I simply register the 5 minutes. Although I may be compromising some accuracy here, it is much more advantageous than having instances inflate the read time significantly.

Once the article page loads, and the expected read time calculations have been made, a timer is readied. The timer is triggered once a user moves the mouse or when the user scrolls the page. This minimizes storing metadata created by bots and to a high extent ensures that a human is sitting behind the screen. Upon navigating away from or closing the article, the read time is then submitted to \textbf{updateReadTime.php} and eventually recorded in the database. This part of the implementation relies on \textbf{window.onbeforeunload} or \textbf{window.onunload}, depending on the browser's user agent. At the time of this writing, this method only for Firefox and Chrome.
 
\vfill

\subsection{Tying it all together}
Now that both the back end and front end code has been implemented, I was able to move on to testing this system. I created a miniature lorem ipsum page which acts as an article being visited. \\

\noindent\includegraphics[scale=1]{img/visit1}

\noindent\ In my PHP code, I populated mock metadata for the article and category. The hit metadata, however, was populated based on the visit data. I went ahead and visited the page for 10 seconds and got the following results:\\

\noindent\includegraphics[scale=0.6]{img/visit2}

\noindent\ My ip address, time of visit, article id (at this point a fake one), timezone, country, region, and read time were all stored in the \textbf{hit} table.\\ \\

\noindent\includegraphics[scale=0.5]{img/visit3}

\noindent\ The article id, category id, article URL, article title, article sample text, and the URL to the article's main pic - were all stored in table \textbf{article}. \\ \\

\noindent\includegraphics[scale=0.7]{img/visit4}

\noindent\ Lastly, the category id and name were both stored in the \textbf{category} table. \\

This concludes the part of the system that implements the data capturing. The next step was to connect this library to a working website. For the scope of this thesis, I implemented the hook to specifically tie into a widely used Content Management System - Wordpress. 

\vfill

\section{Implementation - hooking into Wordpress}
Wordpress calls its articles "posts". The template file for each post is called \textbf{single.php} and is found under the current theme that is being used. One can think of a theme as a set of pre-built PHP, HTML, CSS3, and JavaScript templates that structurally and visually dictate how the website looks and behaves. Thus, in order to accomplish the task of capturing the data from a certain article, I needed to include my capture library inside \textbf{single.php}. However, since my library cannot directly communicate with Wordpress, I had to create a hook that abstracts some of Wordpress's built-in functions. Lastly, I needed to ensure that Wordpress is using jQuery version 2.1.1 or higher. As this is a JavaScript library and that I need available as soon as the page loads, I needed to include it in \textbf{header.php} - which basically comprises the <head></head> portion of a given page. Below is a breakdown of how I hooked into Wordpress.
\subsection{Creating the captureLib folder}
In the interest of portability, I have placed all of my code within a folder called \textbf{captureLib}. If a 3rd party wishes to uses this library in the future, all that would be needed is to simply include \textbf{captureLib} from within \textbf{single.php}. Here is a screenshot of the current directory structure of \textbf{captureLib}. \\

\noindent\includegraphics[scale=0.8]{img/capturelib}    

\newpage

\noindent Below is a breakdown of the current directory structure of the code that captures hits and read times.
\begin{enumerate}
\item \textbf{lib/}
\begin{enumerate}
\item \textbf{constants.php} - contains all the server-side constants that will be used to capture the hits
\item \textbf{db\_connect.php} - connects to the database where the hits are stored
\end{enumerate}
\item \textbf{Article.php, Hit.php, Category.php} - as mentioned previously, these are abstractions that communicate the data from and to the database
\item \textbf{getReadTimeLibrary.php} - a line of code to include the read time JavaScript code if a hit was registered
\begin{lstlisting}[basicstyle=\scriptsize]
<?php 
	/**
		A terse way of including the JavaScript library. The library 
		itself is suffixed with .php instead of .js because there is 
		a php variable defined in the actual JavaScript library
	*/
	if($hit_id) { include("readTime.php"); } 
?>

\item
\end{lstlisting}
\item \textbf{readTime.php} - this is the JavaScript code that captures the read time. It is suffixed with .php (vs. .js) since it requires PHP code to output the hit id within the JavaScript. I have made adjustments to this code to accommodate for users switching tabs while reading a certain article. When the article's window loses focus, the timer stops counting; it only resumes when the user gets back to the article window. This maximizes accuracy in the average read time readings.
\item \textbf{wordpressHook.php} - as mentioned earlier, this is an abstraction into Wordpress's needed built-in functions. In order to hook to different CMSs, the code can be reused. All that would be needed to change is the implementation.
\item \textbf{countries.php} - contains an array that has all country names. Used for database seeding.
\end{enumerate}
\subsection{Creating a mock news website}
In order to test the library, I created a mock news site in Wordpress. In this setup, I built the following categories: Business, Entertainment, Music, Science, Sports, Tech, and World. I tasked a friend to populate mock articles with content that is fitting to the parent category. I created the Wordpress install under my personal web hosting account. The URL to it is http://www.alainibrahim.com/fakenewssite. In the interest of time, I used the default theme that came out of the box. \\

\noindent\includegraphics[scale=0.4]{img/fakenewssite_main} 

\newpage

\subsection{Testing}
\subsubsection{Proof of concept}
Below is the result of my having visited 3 different articles, each for a different amount of time:

\noindent\includegraphics[scale=1]{img/results_categories} \\
\noindent In the \textbf{category} table, the categories metadata of the different articles that I visited were stored in the database. Note that if say I visit an article of the same category again, the category will not be double stored. 

\noindent\includegraphics[scale=0.6]{img/results_articles}
\noindent In the \textbf{article} table, the articles' metadata was stored. The sample text is excerpted by PHP with a default setting of 500 characters. \\ \\ 

\noindent\includegraphics[scale=1]{img/results_hits} \\
\noindent The read times appear to have registered successfully in the \textbf{hit} table. \\

\subsubsection{Seeding mock data}
After testing the capture of data successfully, I needed a way to mimic traffic on a real news website. In addition, I needed to make it appear as if each mock article visit came from a random country of origin. Below are the steps I took to accomplish this:

\begin{enumerate}
\item{Modified \textbf{constants.php} to include a flag that denotes whether the app is in test or production mode:}
\begin{lstlisting}[basicstyle=\scriptsize]
...

// defines whether we are in test or production mode
define(PRODUCTION_ENV,false);
\end{lstlisting}

\item{Modified class \textbf{Hit.php} to produce a fake read time and fake US state of origin if in test mode:}
\begin{lstlisting}[basicstyle=\scriptsize]
...
// if in test environment, populate fake data for read time and country of origin
if(!PRODUCTION_ENV) {
	// get a random time between 10 seconds and 3 minutes
	$read_time = rand(10,180);
	
	// get the list of all the states in the US, and pick one randomly
	include("states.php");
	$region = $states[array_rand($states)];
}
...
\end{lstlisting}
\item{Created \textbf{makeFakeHit.php} to invoke a visit to a random article on my fake news site:}
\begin{lstlisting}[basicstyle=\scriptsize]
/**
	The sole purpose of this file is to visit a random article on the 
	fake news site, causing hits to register
*/

$TESTING_IP = "98.172.153.142";

// 290 is the id of the last fake article that was created
$article_id = rand(1,290);

// cause a fake visit only if testing ip address is registered here
if( $_SERVER['REMOTE_ADDR'] == $TESTING_IP) {
	file_get_contents("http://alainibrahim.com/fakenewssite/?p=" . $article_id);
}
\end{lstlisting}
Note that I have hard coded a testing IP address so that I am the only person who can run this for testing.
\item{Created \textbf{fakeVisit.html} to run on the client side and to call \textbf{makeFakeHit.php} at a pre-specified time interval(using JavaScript):}
\begin{lstlisting}[basicstyle=\scriptsize]
(function(){
	// the amount of time to wait before revisiting the site (in seconds)
	var REVISIT_INTERVAL = 20;

	// trigger the fake visit on the server side
	var makeFakeHit = function() {
		$.get("makeFakeHit.php");
	}
	
	// set a recurrent loop to seed the data
	setInterval(function(){makeFakeHit();console.log("hit issued")},(REVISIT_INTERVAL*1000));
})()
\end{lstlisting}
\end{enumerate}

\noindent Below is a snapshot of the results of the seeding of the database: \\

\noindent\includegraphics[scale=0.8]{img/seeder_mock_hits} \\

\noindent Note that for each hit, the data that is generated by the seeding process is \textbf{article\_id}, \textbf{region}, and \textbf{read\_time}. The other values were left as is because for the scope of this project, they will not be factored into the final visualization. 

\vfill

\section{Implementation - retrieving captured metrics}
At this point, the database was populated with a good amount of data and I needed a way to retrieve it. For this, I created a folder on the server side and called it "vizLib". In this directory I created 3 PHP files that will output results in JSON:
\begin{enumerate}
\item{\textbf{getCategories.php} - outputs all the used article categories.}

\item{\textbf{getCategoriesMetrics.php} - outputs all of the metrics for all categories.}

\item{\textbf{getArticlesMetrics.php} - outputs all of the metrics for all the articles given a category ID, beginning visit time, and ending visit time.}

\item{\textbf{getArticleCountriesMetrics.php} - outputs all of the country metrics for a given article ID.}

\item{\textbf{index.php} - the landing page for the visualization}

\end{enumerate}

\newpage

\section{Implementation - visualization}

For the visualization, I enabled the following tasks for the users:
\begin{itemize}
\item To compare articles' hits and reader attention spans for any period within the last month, given a select category
\item To preview each article's thumbnail, title, and excerpt
\item To contrast the geographic distribution of hits and attention of up to 2 articles
\item To navigate to the full article counterpart on the subject news website
\end{itemize}

\subsection{The controls box}

Typically, controls take up space in the limited real estate area available in any web browser. I decided to have the controls be draggable and minimizeable so they don't negatively impact the user experience. 

\noindent\includegraphics[scale=0.8]{img/viz_3}

\noindent The controls box first partially loads when no category is selected. The "X" hides it; when this is done, the top left gray rectangle with the two white dots is then contoured by a black border - to indicate that it is hidden.

Upon loading the visualization, an AJAX call is made to the server which fetches all the of the news categories into this dropdown box. Once a category is selected, a call is made to the server, which results in all of the data for the selected category to be fetched from the database using the server-side script. The user is notified (via the red colored "Loading..." indicator) of when the data is being loaded:

\noindent\includegraphics[scale=0.8]{img/viz_4}

And, once the data is loaded, the status label turns green and reads "Loaded":

\noindent\includegraphics[scale=0.8]{img/viz_5}

\noindent Below are the descriptions for each component of the controls area:

\begin{itemize}
\item \textbf{Category - } sets the category whose articles the user chooses to see
\item \textbf{Status - } shows the data transfer status of the selected news category
\item \textbf{Show Article Details on Hover - } determines whether a preview of the article will pop up upon hovering over an article in the stacked area chart. 
\item \textbf{Show Article titles - } toggles the visibility of the article titles in the bottom portion of the screen
\item \textbf{Type - } the user gets to select between viewing the number of hits or amount of time spent (in seconds) on the Y axis of the stacked area chart
\item \textbf{Time slider - } This is a dual time slider control in that the user can use it to select one slice (i.e. day) in time or can alternatively select a time range to view the data in 
\end{itemize}

\newpage

\noindent I made the controls box transparent in case the user wants to keep it afloat the visualization: \\

\noindent\includegraphics[scale=0.8]{img/viz_6}



\subsection{Stacked area chart}

Here, the goal was to create a visual product that would allow the user to view all the articles' popularity over time, given a certain news category. I knew that I wanted time to go on the horizontal X axis, and (hits or read time) to go on the Y axis. That said, I first started with a simple line graph, where each line represented an article that was colored differently from the other lines. There were 2 downsides which led me to abandon the line graph: 
\begin{itemize}
\item The lines intersected into a spaghetti mash-up, making it very difficult to spot any significant patterns
\item When a category (e.g. Business) contained more than say 10 articles, I had to use 10 different colors to encode the articles. The issue here was that at a certain point, colors became very similar making it almost impossible to visually query the article all the way through a selected time period 
\end{itemize}

\noindent I instead used the stacked area chart. The stacked area chart maintained a view on the patterns without compromising legibility and supports the following tasks: 
\begin{itemize}
\item To compare articles' hits and reader attention spans for any period within the last month, given a select category
\item To preview an article's thumbnail, title, and excerpt
\end{itemize}

\noindent The X axis is laid out using D3 and represents a time aggregation that occurs on an hourly basis. 

\noindent\includegraphics[scale=0.9]{img/viz_1}

The articles are stacked vertically, each respectively matching its title in the below white space area, which is headed by the selected category. The readying of the data for the stack is done using the D3 area layout, and the visual representation is implemented by using the SVG <path>. The paths by default all contain a fill of light gray. 

Hovering over any article in the stack chart causes both the article's area and corresponding title to turn to steel blue. In reverse, hovering over an article title in the bottom area causes the corresponding article's area in the stack chart to have a fill color of steel blue. 
It is important to note that the user has the option to see a preview of the selected article when checking "Show Article Details On Hover" in the controls box:

\noindent\includegraphics[scale=0.7]{img/viz_2} \\

This results in an overlay to pop over the selected article that includes the article's thumbnail, title, and a 500-character excerpt of the body of the article. 

The user is given the option to contrast the pattern of up 2 articles over time, by left clicking on the select article area. This results in one article being filled with a purple color and the other with a green color. I picked the colors from colorbrewer2.org and ensured they were colorblind safe.

\noindent\includegraphics[scale=0.8]{img/viz_7} \\

\noindent Once the two articles are selected, their corresponding geographic distributions are shown in the right area of the browser. This is covered in the next section. 

\subsection{Geographic bar charts}

In the previous section, two articles are clicked on in the stacked area chart. In addition to their being highlighted in the stacked area chart, a right side pane populates each article's title in its color, along with a table below it. The table contains the geographic origin (e.g. state/country), the total number of hits, and the attention span distribution (i.e. read time in seconds).

\noindent\includegraphics[scale=0.5]{img/viz_8} \\

Whenever a first article is selected, the second article is compared along the same geographic areas for uniformity. That is, only the geographic areas that were included in the first article were displayed for the second article. Clicking the top right "X" removes the article from the right pane, and renders the graph area counterpart back to light gray. 
The hits values are encoded as horizontal bars in dark gray. The attention span column contains a distribution of the captured read times, represented as light gray circles; the amount of seconds spent is encoded by the horizontal position of each circle. The circles are transparent to allow for overlap. The maximum horizontal position of the circle is demarcated by the expected read time. The expected number of seconds was discussed earlier, and is a measure that takes into account the average adult human reading speed and the number of words contained within the article. The total hits bar spans relative to the highest hits value for that article - meaning, the bar that takes up the biggest width is the highest hit value that was registered for that article. 

\vfill

\noindent\includegraphics[scale=0.8]{img/viz_9} \\

\noindent We now examine the geographic area in more detail. Note that automatically, the entries are sorted by the name of the geographic origin, in ascending order. Clicking on the "STATE" header will toggle back and forth the sorting order of the geographic origin name. Similarly, toggling the "TOTAL HITS" header will sort the data in ascending/descending order. When the data is sorted, it is sorted according to the same criterion for both articles. Hovering over a row will cause that row to be highlighted in yellow, and to be contoured in a dashed line solid border. Hovering over any bar will in addition pop up a steel blue overlay which displays - using white text - the number of hits or read time in seconds.  

The geographic bar charts support the following tasks:
\begin{itemize}
\item To contrast the geographic distribution of hits and attention of up to 2 articles
\item To navigate to the full article counterpart on the subject news website
\end{itemize}

\newpage

\section{Results}

\subsection{Mock Website}
The working example that I have discussed so far is based on a mock website that I created using mock visit data.

\subsubsection{Capturing Web Metrics}
\begin{itemize}
\item For each hit, the following metadata was successfully captured: IP address, time visited, article ID, time zone, country of origin, and region of origin
\item For each hit, the read time was captured as a random number between 10 and 180 seconds. When I switched the flag in my PHP code to denote the production environment, the read time was captured successfully when I navigated away from a news article. Most of my testing was done in Google Chrome version 39
\item Each hit was captured according to the revisit threshold time (in seconds) that was specified in the PHP code
\item Each category was captured from Wordpress via the PHP libraries I created
\item Each article's metadata was captured from Wordpress via the PHP libraries I created
\end{itemize}

\subsubsection{Retrieving Web Metrics}
\begin{itemize}
\item The categories were successfully retrieved in JSON via \textbf{getCategories.php}
\item The hits data was retrieved successfully from the MySQL database using JSON via \textbf{getArticlesMetrics.php}. I initially reserved the geographic data for the articles to be serviced by \textbf{getArticleStatesMetrics.php}. However, I felt that in a production environment that many SQL calls to the database server would slow down the system and affect all users. In order to minimize the number of requests to the server, I had the server fetch all data for a given category for the last 31 days. The client-side JavaScript code then parsed and re-organized the fetched data to be used by the interactive visualization
\end{itemize}

\subsubsection{The Interactive Visualization}
\begin{itemize}
\item The final product included a stacked area chart, a list of article titles, and a right pane for geographic analysis of up to 2 articles
\item The hits were first aggregated on a minute to minute basis. Though, this resulted in performance degradation. Adjusting the date range took a delay of about 2-3 seconds to reflect in the redraw of the stacked area chart. I then aggregated on an hourly basis, which ended up plotting Y values over 744 (31 days x 24 hours) X axis time values. This eliminated the delay in the stacked area chart redraw
\item The stacked chart accurately reflected the number of hits and average read time over the selected time range
\item Clicking one or more articles highlighted the article area in the stacked area chart and displayed the total hits based on geography. It also showed the distribution of the recorded read times. For the mock site, the geography was limited to states within the United States
\item The tables that contained the geographic origins were sortable by total hits and by state name - in ascending and descending order
\item When "Show Article Details on Hover" was enabled, the pertinent article's thumbnail image, title, and body excerpt were successfully presented in an overlay
\item Hovering over any row in the geographic hits and attention span table resulted in the row being highlighted in yellow for all articles that are being compared
\item Hovering over a bar representation of the total number of hits invoked a steel blue overlay showing the number of hits
\\
\noindent\includegraphics[scale=0.8]{img/state_hit_hover} \\  

\item Hovering over an attention span circle invoked a steel blue overlay showing the number of seconds that was spent on the article
\noindent\includegraphics[scale=0.8]{img/state_circle_hover} \\  
\end{itemize} 

\subsubsection{Performance}
\begin{itemize}
\item In the mock site, the last 31 days amounted to ~4500 hits per category. The time it took for them to be loaded in the visualization was around 4 seconds, using a PC with an Intel i7 processor and 12 GB of RAM
\item When changing the date range, the stacked chart's redraw time was almost instantaneous. Again, using a PC with an Intel i7 processor and 12 GB of RAM
\item When "Show Article Details on Hover" was enabled, there was a delay of about 500 milliseconds for the image thumbnails to initially show. Once they were cached in the browser's memory, they appeared without delay. For images that were not cached, the delay was about 1 second per megabyte in picture size
\end{itemize}

\newpage

\subsection{Actual Website}
Being a web developer at a news organization, I was able to test the system against one of our mini sites - Afia Darfur (http://www.afiadarfur.com), since it was using Wordpress as the back-end platform. The site targets people in Sudan and keeps them up to date with news specific to Sudan. \\

\noindent\includegraphics[scale=0.4]{img/afiadarfur_main} \\

\subsubsection{Hooking the Custom Libraries into Afia Darfur}
Hooking into the live website involved the following steps:
\begin{enumerate}
\item Creating a database for the web metrics
\item Placing the \textbf{captureLib/} folder in the root folder of the Wordpress theme
\item Including the capture library in Wordpress's \textbf{single.php} 
\item Including the \textbf{vizLib/} folder in the root directory of the website
\item Placing \textbf{updateExpectedReadTime.php} and \textbf{updateReadTime.php} in the root directory of the website 
\end{enumerate}

\subsubsection{Capturing Web Metrics}
\begin{itemize}
\item For each hit, the following metadata was successfully captured: IP address, time visited, article ID, time zone, country of origin, and region of origin
\item  The read time was captured successfully when I navigated away from a news article. Most of my testing was done in Google Chrome version 39
\item Each hit was captured according to the revisit threshold time (in seconds) that was specified in the PHP code. For Afia Darfur, this was set to 20 seconds
\item Each category was captured from Wordpress via the PHP libraries I created
\item Each article's metadata was captured from Wordpress via the PHP libraries I created. In order to ensure that the Arabic text was captured, I had to change the collation of the database tables to utf8\_general\_ci
\item The PHP library files responsible for capturing data had to be encoded in UTF-8
\end{itemize}

\subsubsection{Retrieving Web Metrics}
\begin{itemize}
\item The categories were successfully retrieved in JSON via \textbf{getCategories.php}. The pertinent PHP files had to be encoded in UTF-8 to handle the Arabic characters
\item The hits data was retrieved successfully from the MySQL database using JSON via \textbf{getArticlesMetrics.php}. I initially reserved the geographic data for the articles to be serviced by \textbf{getArticleStatesMetrics.php}. However, I felt that in a production environment that many SQL calls to the database server would slow down the system and affect all users. In order to minimize the number of requests to the server, I had the server fetch all data for a given category for the last 31 days. The client-side JavaScript code then parsed and re-organized the fetched data to be used by the interactive visualization
\end{itemize}

\subsubsection{The Interactive Visualization}
\begin{itemize}
\item The final product included a stacked area chart, a list of article titles, and a right pane for geographic analysis of up to 2 articles
\\ \\
\noindent\includegraphics[scale=0.5]{img/afiadarfur_main_viz} \\
\item The hits were aggregated on an hourly basis resulting in Y values over 744 (31 days x 24 hours) X axis time values
\item The stacked chart accurately reflected the number of hits and average read time over the selected time range
\item Clicking one or more articles highlighted the article encode in the stacked area chart and displayed the total hits based on geography. It also showed the distribution of the recorded read times. Since the target audience of Afia Darfur is an international one, the geographic data was aggregated to countries - unlike in the mock site where units of geography were represented as US states
\item The tables that contained the geographic origins were sortable by total hits and by state name - in ascending and descending order
\item When "Show Article Details on Hover" was enabled, the pertinent article's thumbnail image, title, and body excerpt were successfully presented in an overlay \\ \\
\noindent\includegraphics[scale=0.5]{img/afiadarfur_hover} \\
\\
\item Hovering over any row in the geographic hits and attention span table resulted in the row being highlighted in yellow for all articles that are being compared
\item Hovering over a bar representation of the total number of hits invoked a steel blue overlay showing the number of hits 
\\
\noindent\includegraphics[scale=0.8]{img/country_hit_hover} \\  
\item Hovering over an attention span circle invoked a steel blue overlay showing the number of seconds that was spent on the article
\\
\noindent\includegraphics[scale=0.8]{img/country_hit_hover} \\   
\end{itemize} 


\subsubsection{Performance}
\begin{itemize}
\item In the Afia Darfur website, the last 31 days amounted to ~10,000 hits and ~40,000 hits for the two main categories being used by the web editors. It took approximately 16 seconds to load the ~10,000 hits and 52 seconds to load the ~40,000 hits. 
\item As for adjusting the stacked area graph's time range, there was a ~3-second delay for the ~10,000 hits and a ~10-second delay for the ~40,000 hits. This was done using a PC with an Intel i7 processor and 12 GB of RAM. Again, using a PC with an Intel i7 processor and 12 GB of RAM
\item When "Show Article Details on Hover" was enabled, there was a delay between 50 to 300 milliseconds for the image thumbnails to initially show. Once they were cached in the browser's memory, they appeared without delay. For images that were not cached, the delay was about 1 second per megabyte in picture size
\end{itemize}

\section{Conclusion}
When hooked into the Wordpress website Afiadarfur.com, the capture library that I built registered the hits as intended inside a relational database. It did not register read times for all articles due to factors such as older web browsers not supporting the \textbf{onunload()} method, and due to JavaScript plugins that may have conflicted with the capture library. The data retrieval library that I built successfully downloaded the results of the requested query via JSON. 

As for the interactive visualization, the stacked area chart would have been more effective visualizing less articles at a time. Looking at 300+ articles simultaneously resulted in difficulty when it came to seeing the performance of the less popular articles. For Afiadarfur.com, the web editors needed to spread their articles under several categories - such as business, music, world, and entertainment - as prominent news sites do. Doing this say across 10 categories would have resulted in about 30 articles being represented at a time. Although this would have added clarity, the visualization could have been made even clearer by adding more filters - or, by narrowing the maximum time window into the hits data to about 1 week. Showing the geographic distribution of hits and contrast between two articles was effective. Aggregating the data on an hourly basis resulted in a low visual latency, while maintaining temporal accuracy. 

\newpage

\section{Future Work}
I would have liked to add the following capabilities to my existing visualization:
\begin{itemize}
\item To create a stacked area chart for categories over time, similar to the existing one for articles
\item To visualize each article's interest fluctuations as a relative measure from its publication date. In the current scenario, one is able to see which articles peak and plunge over time. However, it is difficult to contrast two or more articles' fluctuations since their inception date
\item To allow for 4 articles to be simultaneously contrasted across their geographies. I can accomplish this by removing the attention span column and having it as a filter option to narrow the geographic tables
\item To visualize the number of hits and attention spans per capita
\item To capture social media metrics such as Facebook likes and  Twitter tweets, and incorporate them into the stacked area chart 
\item To cache the IP addresses of the visitors in order to minimize calls to the 3rd party API Telize.com
\item To cache query results for each category so as to minimize the request load on the database server
\item To each a hit by a token stored in HTML5's \textbf{LocalStorage} array. In this scenario, if two people visiting the same article are behind the same IP address, then they would be counted as 2 hits instead of 1. Here, if JavaScript is disabled, the measure would then fall back onto that of the IP address - which I did in this thesis 
\end{itemize}
\newpage

\section{Glossary}
\begin{itemize}
\item[] \textbf{Apache HTTP Server (aka Apache)}: A web server software program
\item[] \textbf{Choropleth map}: A thematic map in which areas are shaded or patterned in proportion to the measurement of the statistical variable being displayed on the map (source: Wikipedia).
\item[] \textbf{Class}: In object oriented programming parlance, it refers to the abstraction and representation of a real life object or concept in code.
\item[] \textbf{CSS}: A styling language used to define the aesthetics of elements in a web deliverable. This is implemented in all major web browsers. CSS3 is the most recent version of this language.
\item[] \textbf{D3.js}: A JavaScript library for manipulating documents based on data using HTML, SVG and CSS (source: d3js.org).
\item[] \textbf{Google Analytics}: A service offered by Google that generates detailed statistics about a website's traffic and traffic sources and measures conversions and sales (source: Wikipedia).
\item[] \textbf{HTML 5}: The most recent specification for the markup language that constitutes the visual elements of any web deliverable. It also normally includes some JavaScript and CSS 3 code as part of its implementation.
\item[] \textbf{JavaScript}: A client-side scripting language that is used by all prominent web browsers.
\item[] \textbf{JSON (JavaScript Object Notation)}: A lightweight data-interchange format (source: json.org).
\item[] \textbf{LAMP}: A web development environment comprised of a Linux server, Apache HTTP Server, MySQL, and PHP.
\item[] \textbf{MySQL}: An open source relational database management system.
\item[] \textbf{Object oriented programming}: A programming paradigm that represents real-life elements and concepts as "objects". The implementation involves mimicking only the needed real-life characteristics of the object in code, and calling a working copy of these objects an "instance".
\item[] \textbf{PHP}: An open source server-side scripting language.
\item[] \textbf{Relational database}: A database built on principles of the relational model. Such a database is comprised of tables and fields.
\item[] \textbf{SVG}: An XML-based vector image format for two-dimensional graphics that has support for interactivity and animation (source: Wikipedia).
\item[] \textbf{Web visualization}: A visual deliverable created using web technologies. More than often, it is dynamic in that it visually changes based on the data being fed to it. 
\end{itemize}

\vfill

\begin{thebibliography}{1}

  \bibitem{schumann} Schumann, H. and Muller, W. (2000). {\em Visualisierung - Grundlagen und allgemeine Methoden}. Springer, Berlin, Germany.

  \bibitem{tufte}  Edward R. Tufte, {\em Visual Display of Quantitative Information} (2001).

  \bibitem{munzner} Munzner, T. (2009), {\em A Nested Model for Visualization Design and Validation}.

  \bibitem{cockburn} A. Cockburn, A. Karlson, and B. B. Bederson, {\em "A review of overview+detail, zooming, and focus+context interfaces,"} ACM Computing Surveys (CSUR), vol. 41, no. 1, pp. 1-31, 2008.
  
  \bibitem{playfair} Playfair, W. and Corry, J. (1786). {\em The Commercial and Political Atlas: Representing, by Means of Stained Copper-Plate Charts, the Progress of the Commerce, Revenues, Expenditure and Debts of England during the Whole of the Eighteenth Century.} printed for J. Debrett; G. G. and J. Robinson; J. Sewell; the engraver, S. J. Neele; W. Creech and C. Elliot, Edinburgh; and L. White, London, UK.
  
  \bibitem{frank} Frank, A. U. (1998). {\em Different Types of "Times" in GIS. In Egenhofer, M. J. and Golledge, R. G., editors, Spatial and Temporal Reasoning in Geographic Information Systems}, pages 40-62. Oxford University Press, New York, NY, USA.
  
  \bibitem{goralwalla} Goralwalla, I. A., O zsu, M. T., and Szafron, D. (1998). {\em An Object-Oriented Framework for Temporal Data Models. In Etzion, O. et al., editors, Temporal Databases: Research and Practice}, pages 1-35. Springer, Berlin, Germany.
  
  \bibitem{bettini} Bettini, C., Jajodia, S., and Wang, X. S. (2000). {\em Time Granularities in Databases, Data Mining, and Temporal Reasoning}. Springer, Secaucus, NJ, USA, 1st edition.

  \bibitem{radner} Radner, W., Diendorfer, G., (2014). {\em English sentence optotypes for measuring reading acuity and speed --- the English version of the Radner Reading Charts}

  \bibitem{wills} Graham, W., (2012). {\em Visualizing time: designing graphical representations for statistical data}

  \bibitem{clevelandmcgill} William S. Cleveland, Robert McGill (1984). {\em Graphical perception: Theory, experimentation, and application to the development of graphical methods}
  
  \bibitem{colinware} C.G. Healey (1996). {\em Choosing effective colours for data visualization}  
  
  \bibitem{byronwattenberg} Lee Byron \& Martin Wattenberg (2008) {\em Stacked graphs - Geometry \& Aesthetics}
  
  \end{thebibliography}
 

\end{document}
